{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg-VsdunT-tW"
   },
   "source": [
    "1.run up untill model trainning, upload data.\n",
    "make file called pre_trained_models and move model downloaded into there\n",
    "2.in the models folder, open a model folder <-some_mode->/v1 and copy pipeline from pre_trained_models.config the following:\n",
    "fine_tune_checkpoint_type: \"detection\"\n",
    "batch_size=even number, low\n",
    "\n",
    "fine_tune_checkpoint: \"/content/pre_trained_models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
    "  fine_tune_checkpoint_type: \"detection\"\n",
    "}\n",
    "train_input_reader {\n",
    "  label_map_path: \"/content/data/label_map.txt\"\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"/content/data/train.record/trainAnno.tfrecord\"\n",
    "  }\n",
    "}\n",
    "eval_config {\n",
    "  metrics_set: \"coco_detection_metrics\"\n",
    "  use_moving_averages: false\n",
    "}\n",
    "eval_input_reader {\n",
    "  label_map_path: \"/content/data/label_map.txt\"\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"/content/data/validation.record/trainAnno.tfrecord\"\n",
    "  }\n",
    "}\n",
    "\n",
    "add a txt file into data file called label_map.txt and write \n",
    "item {\n",
    "\tid: 1\n",
    "\tname: \"fire\"\n",
    "}\n",
    "\n",
    "item {\n",
    "\tid: 2\n",
    "\tname: \"natural\"\n",
    "}\n",
    "\n",
    "\n",
    "upload mode_main_tf2.py\n",
    "and \n",
    "exporter_main_v2.py\n",
    "from object_detection from models file i think\n",
    "run command \n",
    "python model_main_tf2.py --pipeline_config_path=.\\models\\ssd_mobilenet_v1\\v1\\pipeline.config --model_dir=.\\models\\ssd_mobilenet_v1\\v1 --checkpoint_every_n=5 --num_workers=1 --alsologtostderr\n",
    "\n",
    "for trainning\n",
    "make folder \"exported_models\" and make another folder with same name as model\n",
    "and run \n",
    "\n",
    "python exporter_main_v2.py --pipeline_config_path=.\\models\\faster_rcnn_inception_resnet_v2\\v1\\pipeline.config --trained_checkpoint_dir=.\\models\\faster_rcnn_inception_resnet_v2\\v1 --output_directory=C:\\Tensorflow\\workspace\\exported_models\\faster_rcnn_inception_resnet_v2 --input_type=image_tensor\n",
    "\n",
    "for eval,\n",
    "\n",
    "then upload juypter notebook and try it out \n",
    "\n",
    "\n",
    "NEXT WE WOULD LIKE TO DOWNLOAD THE MODEL AND RUN WEBCAM TO DETECT FIRE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skippable part which was the initial setting up for the workspace, skip up untill \"Unskippable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJFhY3elhdr3"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree('/content/sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mB3dldSoDbr2",
    "outputId": "f041931e-daeb-451d-a1a8-ea00e08c941b"
   },
   "outputs": [],
   "source": [
    "!gdown http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOA4z9lNiEoh"
   },
   "outputs": [],
   "source": [
    "mkdir workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKKP4jT_iJKn",
    "outputId": "70191ae9-a581-4f60-8158-9c9bde50c5fd"
   },
   "outputs": [],
   "source": [
    "cd workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_IRYdyVqnong"
   },
   "outputs": [],
   "source": [
    "mkdir exported_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruuozUbnotE4",
    "outputId": "f499e157-89b0-4571-ed61-1a80a77261b3"
   },
   "outputs": [],
   "source": [
    "cd exported_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC85WjmBoP_0"
   },
   "outputs": [],
   "source": [
    "mkdir ssd_mobilenet_v2_fpnlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBN4DYJpoxXx",
    "outputId": "eea8f0be-9003-4880-d78f-1f0d03c7c300"
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vySEm8pegsLs"
   },
   "outputs": [],
   "source": [
    "mkdir pre_trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amMCO26gHvd1"
   },
   "outputs": [],
   "source": [
    "mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDf-dYJzIH6C",
    "outputId": "fee168b7-0ebb-4ab2-916a-d9f975e1b10e"
   },
   "outputs": [],
   "source": [
    "cd models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvVMNkkiIP43"
   },
   "outputs": [],
   "source": [
    "mkdir ssd_mobilenet_v2_fpnlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DDxysc4IRdx",
    "outputId": "60cccdee-9667-4ee8-ebd3-2965de94f75f"
   },
   "outputs": [],
   "source": [
    "cd ssd_mobilenet_v2_fpnlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lz-VHoAVIS6Z"
   },
   "outputs": [],
   "source": [
    "mkdir v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srbIKEd6VVD-",
    "outputId": "5b03ae1a-ed72-4e91-fa4d-90dcc50f58e3"
   },
   "outputs": [],
   "source": [
    "cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKdNoY4zDqGS",
    "outputId": "d49d6a00-a850-4f0e-af6a-59a73ef65ecf"
   },
   "outputs": [],
   "source": [
    "#!tar -z ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\n",
    "!tar -xzvf \"ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.tar.gz\" -C \"/content/workspace/pre_trained_models\"     #[run this cell to extract tar.gz files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-t-0O2eVa19"
   },
   "source": [
    "# Upload data rfrecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NiWgyAR7lpqK",
    "outputId": "bf917267-d2f3-4876-aa46-536132604582"
   },
   "outputs": [],
   "source": [
    "cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jkWaFYDeghi",
    "outputId": "418d4abf-3baa-4aba-95e5-cb7e43b757bc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzSEnWaQesF9"
   },
   "outputs": [],
   "source": [
    "cp /content/gdrive/MyDrive/data.zip /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZPkc-3kpC1K"
   },
   "outputs": [],
   "source": [
    "cp /content/gdrive/MyDrive/exporter_main_v2.py /content/workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE4Uwt07pc0-"
   },
   "outputs": [],
   "source": [
    "cp /content/gdrive/MyDrive/model_main_tf2.py /content/workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpOsR97epoBB"
   },
   "outputs": [],
   "source": [
    "cp /content/gdrive/MyDrive/pipeline.config /content/workspace/models/ssd_mobilenet_v2_fpnlite/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUjbcCqYSj9v",
    "outputId": "347eab4e-1eee-4d1f-aa3b-dbae481f8349"
   },
   "outputs": [],
   "source": [
    "!unzip data.zip -d /content/workspace "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8qv0-2xDdwl"
   },
   "source": [
    "# We have model we trained before that we're going to use with better configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp /content/gdrive/MyDrive/mode_v1.zip /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip mode_v1.zip -d /content/workspace/models/ssd_mobilenet_v2_fpnlite/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unskippable part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Mosa-T/Project_deep_learning.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv /content/Project_deep_learning/workspace /content/ \\\n",
    "mv /content/Project_deep_learning/DL_project_v3.ipynb /content/ \\\n",
    "mv /content/Project_deep_learning/input.mp4 /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('/content/Project_deep_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "msLr_C_AMxzL",
    "outputId": "197dc738-033d-49ff-fe32-c775daed98a8"
   },
   "outputs": [],
   "source": [
    "!pip install lvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-35R6ZwPmcF",
    "outputId": "450f912d-aa43-4806-c396-0975fe6d551f"
   },
   "outputs": [],
   "source": [
    "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
    "!pip install Cython tf_slim\n",
    "!git clone https://github.com/tensorflow/models.git\n",
    "%cd /content/models/research\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "%set_env PYTHONPATH=/content/models/research:/content/models/research/slim\n",
    "import os\n",
    "os.environ['PYTHONPATH'] += \":/content/models\"\n",
    "import sys\n",
    "sys.path.append(\"/content/models\")\n",
    "!python object_detection/builders/model_builder_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtZSJTMBRSSJ",
    "outputId": "47baa371-ad5a-4dc9-d6bc-2d7f553f1316"
   },
   "outputs": [],
   "source": [
    "pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "resixTnMschi",
    "outputId": "1b35ca2b-5301-4b06-f3bb-0ad6caaab9ee"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kN49g3HztAUr"
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mosa-T\\Desktop\\content\\workspace\n"
     ]
    }
   ],
   "source": [
    "cd /content/workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fuTixRA3KQ_a",
    "outputId": "d334d30b-0783-4cb3-8d4d-7ffc72acd4e8"
   },
   "outputs": [],
   "source": [
    "!python model_main_tf2.py --pipeline_config_path=/content/workspace/models/ssd_mobilenet_v2_fpnlite/v1/pipeline.config --model_dir=/content/workspace/models/ssd_mobilenet_v2_fpnlite/v1 --checkpoint_every_n=5 --num_workers=2 --alsologtostderr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "73-gIJV7Rq6m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"exporter_main_v2.py\", line 105, in <module>\n",
      "    from object_detection import exporter_lib_v2\n",
      "ModuleNotFoundError: No module named 'object_detection'\n"
     ]
    }
   ],
   "source": [
    "!python exporter_main_v2.py --pipeline_config_path=/content/workspace/models/ssd_mobilenet_v2_fpnlite/v1/pipeline.config --trained_checkpoint_dir=/content/workspace/models/ssd_mobilenet_v2_fpnlite/v1 --output_directory=/content/workspace/exported_models/ssd_mobilenet_v2_fpnlite --input_type=image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqCx6qcfKTMc"
   },
   "source": [
    "# Validation model - logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python model_main_tf2.py --pipeline_config_path=/content/workspace/models/ssd_mobilenet_v2_fpnlite/v1/pipeline.config --model_dir=/content/workspace/models/ssd_mobilenet_v2_fpnlite/v1/ --checkpoint_dir=/content/workspace/models/ssd_mobilenet_v2_fpnlite/v1/ --num_workers=2 --sample_1_of_n_eval_examples=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir='C:/Users/Mosa-T/Desktop/content/workspace/models/ssd_mobilenet_v2_fpnlite/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard dev upload --logdir 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDQrsEaWvJ74"
   },
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2BVDL-XSj9y"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install tqdm\n",
    "!pip install tf_slim\n",
    "!pip install scipy\n",
    "!pip install tf-models-official\n",
    "!pip install pyqt5\n",
    "!pip install mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hbd4LA65bJxk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path2scripts = './models/research/' #provide pass to the research folder\n",
    "sys.path.insert(0, path2scripts) # making scripts in models/research available for import\n",
    "import numpy as np\n",
    "import time\n",
    "import numpy as np\n",
    "import mmcv, cv2\n",
    "from threading import Thread\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "import smtplib, ssl\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.image import MIMEImage\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q33tEmFFkbYu",
    "outputId": "99b607e8-9fb2-413e-881b-cdd810af0e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "#tf.config.threading.set_intra_op_parallelism_threads(100)\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(100)\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "biudoAc4krEA"
   },
   "outputs": [],
   "source": [
    "#path2config ='/content/workspace/exported_models/ssd_mobilenet_v2_fpnlite/pipeline.config'\n",
    "#path2model = '/content/workspace/exported_models/ssd_mobilenet_v2_fpnlite/checkpoint'\n",
    "path2config ='C:/Users/Mosa-T/Desktop/content/workspace/exported_models/ssd_mobilenet_v2_fpnlite/pipeline.config'\n",
    "path2model = 'C:/Users/Mosa-T/Desktop/content/workspace/exported_models/ssd_mobilenet_v2_fpnlite/checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Pv9UXJvRk5d9"
   },
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file(path2config) # importing config\n",
    "model_config = configs['model'] # recreating model config\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False) # importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3yJi3rOk7-g",
    "outputId": "cbf7a1f1-7624-4ba7-ddfd-b944a6b0ad89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2698e7f1730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(path2model, 'ckpt-0')).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ekjg1bmIlPXm"
   },
   "outputs": [],
   "source": [
    "path2label_map = 'C:/Users/Mosa-T/Desktop/content/workspace/data/label_map.txt' \n",
    "#path2label_map = '/content/workspace/data/label_map.txt' \n",
    "category_index = label_map_util.create_category_index_from_labelmap(path2label_map,use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qMsqoOTylWrJ"
   },
   "outputs": [],
   "source": [
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def framePred(img):\n",
    "    image_np=img.copy()\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes']+label_id_offset,\n",
    "            detections['detection_scores'],\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=10,\n",
    "            min_score_thresh=0.25,\n",
    "            agnostic_mode=False,\n",
    "            line_thickness=2)\n",
    "    return image_np_with_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email_warning(fire_image):\n",
    "    sender_email = \"firedetectionmobilenet@gmail.com\"\n",
    "    receiver_email = \"firedetectionmobilenet@gmail.com\"\n",
    "    #password = input(\"Type your password and press enter:\")\n",
    "    password = ('123qwe!@#qwe')\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] = \"Fire detected!\"\n",
    "    message[\"From\"] = sender_email\n",
    "    message[\"To\"] = receiver_email\n",
    "\n",
    "    # Create the plain-text and HTML version of your message\n",
    "    text = \"\"\"\\\n",
    "    Hi,\n",
    "    We've detected a fire, please check the image to confirm\"\"\"\n",
    "    html = \"\"\"\\\n",
    "    <html>\n",
    "      <body>\n",
    "        <p>Hi,<br>\n",
    "           <br>\n",
    "           We've detected a fire, please check the image to confirm \n",
    "        </p>\n",
    "        <b><br><img src=\"cid:image1\"><br>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Turn these into plain/html MIMEText objects\n",
    "    part1 = MIMEText(text, \"plain\")\n",
    "    part2 = MIMEText(html, \"html\")\n",
    "\n",
    "    # Add HTML/plain-text parts to MIMEMultipart message\n",
    "    # The email client will try to render the last part first\n",
    "    message.attach(part1)\n",
    "    message.attach(part2)\n",
    "\n",
    "\n",
    "    #Attach the image\n",
    "    fp = open(fire_image, 'rb')\n",
    "    msgImage = MIMEImage(fp.read())\n",
    "    fp.close()\n",
    "    msgImage.add_header('Content-ID', '<image1>')\n",
    "    message.attach(msgImage)\n",
    "\n",
    "\n",
    "    # Create secure connection with server and send email\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(\n",
    "            sender_email, receiver_email, message.as_string()\n",
    "        )\n",
    "        server.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def framePred_email(img,email_flag):\n",
    "    image_np=img.copy()\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "    \n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes']+label_id_offset,\n",
    "            detections['detection_scores'],\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=10,\n",
    "            min_score_thresh=0.25,\n",
    "            agnostic_mode=False,\n",
    "            line_thickness=2)\n",
    "    #print(detections['detection_boxes'])\n",
    "    if not email_flag and len(detections['detection_boxes']):\n",
    "        print('fire detected, sending email!')\n",
    "        im = Image.fromarray(image_np_with_detections)\n",
    "        im.save(\"fire_detected.jpg\")\n",
    "        send_email_warning(\"fire_detected.jpg\")\n",
    "        email_flag = True\n",
    "    return image_np_with_detections,email_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2J8xtS4Flbyl"
   },
   "outputs": [],
   "source": [
    "def plot_detection(path2images, box_th=0.25):\n",
    "    for image_path in path2images:\n",
    "\n",
    "        print('Running inference for {}... '.format(image_path), end='')\n",
    "\n",
    "        image_np = np.array(Image.open(image_path))\n",
    "        \n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        # All outputs are batches tensors.\n",
    "        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "        # We're only interested in the first num_detections.\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                      for key, value in detections.items()}\n",
    "        \n",
    "        detections['num_detections'] = num_detections\n",
    "\n",
    "        # detection_classes should be ints.\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "        image_np_with_detections = image_np.copy()\n",
    "\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=200,\n",
    "                min_score_thresh=box_th,\n",
    "                agnostic_mode=False,\n",
    "                line_thickness=2)\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.imshow(image_np_with_detections)\n",
    "        print('Done')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ohYAF3ABld28"
   },
   "outputs": [],
   "source": [
    "def nms(rects, thd=0.5):\n",
    "    out = []\n",
    "    remove = [False] * len(rects)\n",
    "    for i in range(0, len(rects) - 1):\n",
    "        if remove[i]:\n",
    "            continue\n",
    "        inter = [0.0] * len(rects)\n",
    "        for j in range(i, len(rects)):\n",
    "            if remove[j]:\n",
    "                continue\n",
    "            inter[j] = intersection(rects[i][0], rects[j][0]) / min(square(rects[i][0]), square(rects[j][0]))\n",
    "\n",
    "        max_prob = 0.0\n",
    "        max_idx = 0\n",
    "        for k in range(i, len(rects)):\n",
    "            if inter[k] >= thd:\n",
    "                if rects[k][1] > max_prob:\n",
    "                    max_prob = rects[k][1]\n",
    "                    max_idx = k\n",
    "\n",
    "        for k in range(i, len(rects)):\n",
    "            if (inter[k] >= thd) & (k != max_idx):\n",
    "                remove[k] = True\n",
    "\n",
    "    for k in range(0, len(rects)):\n",
    "        if not remove[k]:\n",
    "            out.append(rects[k])\n",
    "\n",
    "    boxes = [box[0] for box in out]\n",
    "    scores = [score[1] for score in out]\n",
    "    classes = [cls[2] for cls in out]\n",
    "    return boxes, scores, classes\n",
    "\n",
    "\n",
    "def intersection(rect1, rect2):\n",
    "\n",
    "    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n",
    "    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n",
    "    overlapArea = x_overlap * y_overlap;\n",
    "    return overlapArea\n",
    "\n",
    "\n",
    "def square(rect):\n",
    "\n",
    "    return abs(rect[2] - rect[0]) * abs(rect[3] - rect[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1uqwj2RblgDB"
   },
   "outputs": [],
   "source": [
    "def raw_output(path2images,box_th = 0.25,nms_th = 0.5,to_file = False,data = None, path2dir = False):\n",
    "    \n",
    "    print (f'Current data set is {data}')\n",
    "    print (f'Ready to start inference on {len(path2images)} images!')\n",
    "    \n",
    "    for image_path in tqdm(path2images):\n",
    "        \n",
    "        if path2dir: # if a path to a directory where images are stored was passed in\n",
    "            image_path = os.path.join(path2dir, image_path.strip())\n",
    "            \n",
    "        image_np = np.array(Image.open(image_path))\n",
    "\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "        \n",
    "        # checking how many detections we got\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        \n",
    "        # filtering out detection in order to get only the one that are indeed detections\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        \n",
    "        # detection_classes should be ints.\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "        \n",
    "        # defining what we need from the resulting detection dict that we got from model output\n",
    "        key_of_interest = ['detection_classes', 'detection_boxes', 'detection_scores']\n",
    "        \n",
    "        # filtering out detection dict in order to get only boxes, classes and scores\n",
    "        detections = {key: value for key, value in detections.items() if key in key_of_interest}\n",
    "        \n",
    "        if box_th: # filtering detection if a confidence threshold for boxes was given as a parameter\n",
    "            for key in key_of_interest:\n",
    "                scores = detections['detection_scores']\n",
    "                current_array = detections[key]\n",
    "                filtered_current_array = current_array[scores > box_th]\n",
    "                detections[key] = filtered_current_array\n",
    "        \n",
    "        if nms_th: # filtering rectangles if nms threshold was passed in as a parameter\n",
    "            # creating a zip object that will contain model output info as\n",
    "            output_info = list(zip(detections['detection_boxes'],\n",
    "                                   detections['detection_scores'],\n",
    "                                   detections['detection_classes']\n",
    "                                  )\n",
    "                              )\n",
    "            boxes, scores, classes = nms(output_info)\n",
    "            \n",
    "            detections['detection_boxes'] = boxes # format: [y1, x1, y2, x2]\n",
    "            detections['detection_scores'] = scores\n",
    "            detections['detection_classes'] = classes\n",
    "            \n",
    "        if to_file and data: # if saving to txt file was requested\n",
    "\n",
    "            image_h, image_w, _ = image_np.shape\n",
    "            file_name = f'pred_result_{data}.txt'\n",
    "            \n",
    "            line2write = list()\n",
    "            line2write.append(os.path.basename(image_path))\n",
    "            \n",
    "            with open(file_name, 'a+') as text_file:\n",
    "                # iterating over boxes\n",
    "                for b, s, c in zip(boxes, scores, classes):\n",
    "                    \n",
    "                    y1abs, x1abs = b[0] * image_h, b[1] * image_w\n",
    "                    y2abs, x2abs = b[2] * image_h, b[3] * image_w\n",
    "                    \n",
    "                    list2append = [x1abs, y1abs, x2abs, y2abs, s, c]\n",
    "                    line2append = ','.join([str(item) for item in list2append])\n",
    "                    \n",
    "                    line2write.append(line2append)\n",
    "                \n",
    "                line2write = ' '.join(line2write)\n",
    "                text_file.write(line2write + os.linesep)\n",
    "        \n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idfC-8Hulhld",
    "outputId": "7967db48-9d3f-4aaf-f2c4-a917a85c02eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current data set is None\n",
      "Ready to start inference on 1 images!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'detection_boxes': [array([0.2380696 , 0.41737288, 0.68020064, 0.6887825 ], dtype=float32)],\n",
       " 'detection_scores': [0.92163604],\n",
       " 'detection_classes': [0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_output(path2images=['image_39.jpg'],data = None, path2dir = 'C:/Users/Mosa-T/Desktop/content/workspace/data/test.record/')\n",
    "#raw_output(path2images=['image_39.jpg'],data = None, path2dir = '/content/workspace/data/test.record/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "O5xXtPVkloDi",
    "outputId": "cd485d24-e135-4698-f28d-8d91571f3ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for C:/Users/Mosa-T/Desktop/content/workspace/data/test.record/image_39.jpg... Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-cd0e8883bd39>:41: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show();\n"
     ]
    }
   ],
   "source": [
    "plot_detection(['C:/Users/Mosa-T/Desktop/content/workspace/data/test.record/image_39.jpg'])\n",
    "#plot_detection(['/content/workspace/data/test.record/image_39.jpg'])\n",
    "#if it doesn't plot then remove %matlabplot inline, run cell and add it back, run again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to train if the results are acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8wVIINVrej4",
    "outputId": "883abb07-aa01-475d-e51d-3e5b4b845a38"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5h6CV6mqFj5"
   },
   "outputs": [],
   "source": [
    "#!zip -r /content/file.zip /content/workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp /content/file.zip /content/gdrive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcrsukFY__Sd"
   },
   "source": [
    "# Main event - Run locally - Colab does a terrible job at playing/displaying videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time detection - Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real time detection\n",
    "cap = cv2.VideoCapture(-1)\n",
    "cap.read()\n",
    "#cap = cv2.VideoCapture('C:/Users/Mosa-T/Desktop/content/workspace/data/test.record/video1.mp4')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _,img = cap.read()\n",
    "    detections = framePred(img)\n",
    "    cv2.imshow('object detection',detections)\n",
    "    key=cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire detection of stored videos can be ran on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame,press q to stop detection: 1267\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#downloaded video fire detection\n",
    "video = mmcv.VideoReader('C:/Users/Mosa-T/Desktop/content/workspace/data/test.record/output.mp4')\n",
    "#video = mmcv.VideoReader('/content/workspace/data/test.record/output.mp4')\n",
    "#frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "frames = [(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "\n",
    "frames_tracked = []\n",
    "for i, frame in enumerate(frames):\n",
    "    print('\\rTracking frames: {}'.format(i + 1), end='')\n",
    "    frame = framePred(frame)\n",
    "    frames_tracked.append(frame)\n",
    "print('\\nDone') \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save detection video\n",
    "dim = Image.fromarray(frames_tracked[0]).size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
    "video_tracked = cv2.VideoWriter('video_tracked8.mp4', fourcc, 25.0, dim)\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n",
    "video_tracked.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play saved video\n",
    "cap = cv2.VideoCapture('C:/Users/Mosa-T/Desktop/content/video_tracked7.mp4')\n",
    "   \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video  file\")\n",
    "\n",
    "#Read until video is completed\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow('Frame', frame)\n",
    "    else: \n",
    "        break\n",
    "        \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "#When everything done, release \n",
    "# the video capture object\n",
    "cap.release()\n",
    "   \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time detection of stored video with email sending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frames: fire detected, sending email!\n",
      "Tracking frames: \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Downloaded video fire detection - display rather than storing\n",
    "video = mmcv.VideoReader('C:/Users/Mosa-T/Desktop/content/workspace/data/test.record/video1.mp4')\n",
    "#video = mmcv.VideoReader(0)\n",
    "\n",
    "#frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "frames = [(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "frames_tracked = []\n",
    "mailed_flag = False\n",
    "for i, frame in enumerate(frames):\n",
    "    print('\\rTracking frames: '.format(i + 1), end='')\n",
    "    frame_draw,mailed_flag = framePred_email(frame,mailed_flag)\n",
    "    key=cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    #frames_tracked.append(frame_draw)\n",
    "    cv2.imshow('detection fire',cv2.cvtColor(np.array(frame_draw), cv2.COLOR_RGB2BGR))\n",
    "print('\\nDone') \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempts at speeding up procceses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying multithreading for real-time \n",
    "class VideoStreamWidget(object):\n",
    "    def __init__(self, src=0):\n",
    "        self.capture = cv2.VideoCapture(src)\n",
    "        # Start the thread to read frames from the video stream\n",
    "        self.thread = Thread(target=self.update, args=())\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "\n",
    "    def update(self):\n",
    "        # Read the next frame from the stream in a different thread\n",
    "        while True:\n",
    "            if self.capture.isOpened():\n",
    "                (self.status, self.frame) = self.capture.read()\n",
    "            time.sleep(.01)\n",
    "\n",
    "    def show_frame(self):\n",
    "        # Display frames in main program\n",
    "        self.frame=framePred(self.frame)\n",
    "        cv2.imshow('frame', self.frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            self.capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    video_stream_widget = VideoStreamWidget()\n",
    "    while True:\n",
    "        try:\n",
    "            video_stream_widget.show_frame()\n",
    "        except AttributeError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing - Sadly windows support for multiprocessing is very messy and buggy so we did not go through with fixing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: multiprocess in c:\\users\\mosa-t\\anaconda3\\envs\\aaaaaaaaa\\lib\\site-packages (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in c:\\users\\mosa-t\\anaconda3\\envs\\aaaaaaaaa\\lib\\site-packages (from multiprocess) (0.3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import time\n",
    "import subprocess as sp\n",
    "#import multiprocessing as mp\n",
    "#mp = mp.set_start_method('fork')\n",
    "import multiprocess as mp\n",
    "from os import remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frame_details(file_name):\n",
    "    cv2video = cv.VideoCapture(file_name)\n",
    "    height = cv2video.get(cv.CAP_PROP_FRAME_HEIGHT)\n",
    "    width  = cv2video.get(cv.CAP_PROP_FRAME_WIDTH) \n",
    "    framecount = cv2video.get(cv.CAP_PROP_FRAME_COUNT ) \n",
    "    return width,height,framecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_output_files(num_processes):\n",
    "    # Create a list of output files and store the file names in a txt file\n",
    "    list_of_output_files = [\"output_{}.mp4\".format(i) for i in range(num_processes)]\n",
    "    with open(\"list_of_output_files.txt\", \"w\") as f:\n",
    "        for t in list_of_output_files:\n",
    "            f.write(\"file {} \\n\".format(t))\n",
    "\n",
    "    # use ffmpeg to combine the video output files\n",
    "    ffmpeg_cmd = \"ffmpeg -y -loglevel error -f concat -safe 0 -i list_of_output_files.txt -vcodec copy \" + output_file_name\n",
    "    sp.Popen(ffmpeg_cmd, shell=True).wait()\n",
    "\n",
    "    # Remove the temperory output files\n",
    "    for f in list_of_output_files:\n",
    "        remove(f)\n",
    "    remove(\"list_of_output_files.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video():\n",
    "    # Read video file\n",
    "    cap = cv.VideoCapture(file_name)\n",
    "\n",
    "    # get height, width and frame count of the video\n",
    "    width, height = (\n",
    "            int(cap.get(cv.CAP_PROP_FRAME_WIDTH)),\n",
    "            int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "        )\n",
    "    fps = int(cap.get(cv.CAP_PROP_FPS))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv.VideoWriter_fourcc(*'FMP4')\n",
    "    out = cv.VideoWriter()\n",
    "    output_file_name = \"output_single.mp4\"\n",
    "    out.open(output_file_name, fourcc, fps, (width, height), True)\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            #print(ret)\n",
    "            if not ret:\n",
    "                break\n",
    "            im = frame\n",
    "            # Perform detection of frame\n",
    "            im = framePred(im)\n",
    "            #print(ret)\n",
    "            # write the frame\n",
    "            out.write(im)\n",
    "    except:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        \n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video frame count = 1267.0\n",
      "Width = 1280.0, Height = 720.0\n",
      "Video processing using single process...\n",
      "Time taken: 451.40186882019043\n",
      "FPS : 2.806811596308857\n"
     ]
    }
   ],
   "source": [
    "def single_process():\n",
    "    print(\"Video processing using single process...\")\n",
    "    start_time = time.time()\n",
    "    process_video()\n",
    "    end_time = time.time()\n",
    "    total_processing_time = end_time - start_time\n",
    "    print(\"Time taken: {}\".format(total_processing_time))\n",
    "    print(\"FPS : {}\".format(frame_count/total_processing_time))\n",
    "    \n",
    "file_name = \"C:/Users/Mosa-T/Desktop/content/input.mp4\"\n",
    "output_file_name = \"output_single.mp4\"\n",
    "width, height, frame_count = get_video_frame_details(file_name)\n",
    "print(\"Video frame count = {}\".format(frame_count))\n",
    "print(\"Width = {}, Height = {}\".format(width, height))\n",
    "single_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_multiprocessing(group_number):\n",
    "    # Read video file\n",
    "    import cv2 as cv\n",
    "    file_name = \"input.mp4\"\n",
    "    output_file_name = \"output.mp4\"\n",
    "    width, height, frame_count = get_video_frame_details(file_name)\n",
    "    print(\"Video frame count = {}\".format(frame_count))\n",
    "    print(\"Width = {}, Height = {}\".format(width, height))\n",
    "    num_processes = mp.cpu_count()\n",
    "    print(\"Number of CPU: \" + str(num_processes))\n",
    "    frame_jump_unit =  frame_count// num_processes\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    cap = cv.VideoCapture(file_name)\n",
    "    cap.set(cv.CAP_PROP_POS_FRAMES, frame_jump_unit * group_number)\n",
    "    # get height, width and frame count of the video\n",
    "    width, height = (\n",
    "            int(cap.get(cv.CAP_PROP_FRAME_WIDTH)),\n",
    "            int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "        )\n",
    "    no_of_frames = int(cap.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv.CAP_PROP_FPS))\n",
    "    proc_frames = 0\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv.VideoWriter_fourcc(*'FMP4')\n",
    "    out = cv.VideoWriter()\n",
    "    output_file_name = \"output_multi.mp4\"\n",
    "    out.open(\"output_{}.mp4\".format(group_number), fourcc, fps, (width, height), True)\n",
    "    try:\n",
    "        while proc_frames < frame_jump_unit:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            im = frame\n",
    "            # Perform face detection on each frame\n",
    "            im = framePred(im).copy()\n",
    "            # Loop through list (if empty this will be skipped) and overlay green bboxes\n",
    "            # write the frame\n",
    "            out.write(im)\n",
    "            proc_frames += 1\n",
    "    except:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_process():\n",
    "    num_processes = mp.cpu_count()\n",
    "    print(\"Video processing using {} processes...\".format(num_processes))\n",
    "    start_time = time.time()\n",
    "    import cv2 as cv\n",
    "    file_name = \"input.mp4\"\n",
    "    output_file_name = \"output.mp4\"\n",
    "    width, height, frame_count = get_video_frame_details(file_name)\n",
    "    print(\"Video frame count = {}\".format(frame_count))\n",
    "    print(\"Width = {}, Height = {}\".format(width, height))\n",
    "    num_processes = mp.cpu_count()\n",
    "    print(\"Number of CPU: \" + str(num_processes))\n",
    "    frame_jump_unit =  frame_count// num_processes\n",
    "\n",
    "    # Paralle the execution of a function across multiple input values\n",
    "    p = mp.Pool(num_processes)\n",
    "    print(num_processes)\n",
    "    p.map(process_video_multiprocessing, range(num_processes))\n",
    "    print('b')\n",
    "    combine_output_files(num_processes)\n",
    "    print('c')\n",
    "    end_time = time.time()\n",
    "   \n",
    "    total_processing_time = end_time - start_time\n",
    "    print(\"Time taken: {}\".format(total_processing_time))\n",
    "    print(\"FPS : {}\".format(frame_count/total_processing_time))\n",
    "\n",
    "#file_name = \"input.mp4\"\n",
    "#output_file_name = \"output.mp4\"\n",
    "#width, height, frame_count = get_video_frame_details(file_name)\n",
    "#print(\"Video frame count = {}\".format(frame_count))\n",
    "#print(\"Width = {}, Height = {}\".format(width, height))\n",
    "#num_processes = mp.cpu_count()\n",
    "#print(\"Number of CPU: \" + str(num_processes))\n",
    "#frame_jump_unit =  frame_count// num_processes\n",
    "#multi_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video frame count = 1267.0\n",
      "Width = 1280.0, Height = 720.0\n",
      "Number of CPU: 12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    file_name = \"input.mp4\"\n",
    "    output_file_name = \"output.mp4\"\n",
    "    width, height, frame_count = get_video_frame_details(file_name)\n",
    "    print(\"Video frame count = {}\".format(frame_count))\n",
    "    print(\"Width = {}, Height = {}\".format(width, height))\n",
    "    num_processes = mp.cpu_count()\n",
    "    print(\"Number of CPU: \" + str(num_processes))\n",
    "    frame_jump_unit =  frame_count// num_processes\n",
    "    # Paralle the execution of a function across multiple input values\n",
    "    p = mp.Pool(num_processes)\n",
    "    print(num_processes)\n",
    "    p.map(process_video_multiprocessing, range(num_processes))\n",
    "    print('b')\n",
    "    combine_output_files(num_processes)\n",
    "    print('c')\n",
    "    end_time = time.time()\n",
    "   \n",
    "    total_processing_time = end_time - start_time\n",
    "    print(\"Time taken: {}\".format(total_processing_time))\n",
    "    print(\"FPS : {}\".format(frame_count/total_processing_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful code - recording, playbacks and delay videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record and playback for webcam.\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def rec():\n",
    "    # record a video\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'FMP4')\n",
    "    out = cv2.VideoWriter('output.mp4',fourcc, 30.0, (640,480))\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret==True:\n",
    "            out.write(frame)\n",
    "            cv2.imshow('frame',frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "def playback():\n",
    "    # play a video\n",
    "    cap = cv2.VideoCapture('output.mp4')\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            cv2.imshow('frame',frame)\n",
    "        else:\n",
    "            break\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# create empty window to process input\n",
    "\n",
    "cv2.namedWindow('frame')\n",
    "# main loop\n",
    "while True:\n",
    "    key = cv2.waitKey(10) \n",
    "    if key == ord('r'):\n",
    "        rec()\n",
    "    elif key == ord('p'):\n",
    "        playback()\n",
    "    elif key == ord('x'):\n",
    "        # end main loop\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webcam with delay\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(cap.get(3))\n",
    "print(cap.get(4))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "DELAY_SECONDS = 5\n",
    "frames = []\n",
    "\n",
    "DELAY_SECONDS = 5\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    frame = framePred(frame)\n",
    "    frames.append(frame)\n",
    "    if time.time() - start_time > DELAY_SECONDS:\n",
    "        cv2.imshow(\"frame\", frames.pop(0))\n",
    "\n",
    "    key = cv2.waitKey(60)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows() \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL_project_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
